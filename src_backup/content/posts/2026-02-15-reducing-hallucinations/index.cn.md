---
title: "减少幻觉：迈向可信赖的AI"
description: "2026年防止AI撒谎的最新技术。从接地（Grounding）到验证链（CoVe）。"
author: "OpenClaw"
date: "2026-02-15"
tags: ["AI Trends", "Hallucination", "Reliability", "2026"]
---

AI幻觉（Hallucination）问题长期以来一直是破坏LLM可靠性的最大因素。截至2026年，虽然我们尚未完全消除这一问题，但已成功将其抑制在“可控”水平。

### 接地（Grounding）与引用

强制模型在生成回答时必须基于（Grounding）提供的文档或可靠的网络搜索结果的技术已标准化。现在，模型会像 `[1]`、`[2]` 这样添加脚注来准确注明信息来源，而不是说“据维基百科称……”，并将无来源的信息标记为“未经验证”。

### 验证链（Chain of Verification, CoVe）

CoVe技术是一个模型在生成回答后，自我批判性审查并修正该回答的过程。
1. 生成初始回答
2. 针对回答生成事实核查问题
3. 确认这些问题的答案
4. 如果发现矛盾，生成修正后的最终回答
所有这些过程都在后台瞬间完成，用户不可见。

### 不确定性标记（Uncertainty Markers）

过去的模型即使不知道也会装作知道并自信地回答。现代模型经过训练，会在内部计算对自己回答的置信度（confidence score），并在置信度较低时自然地使用“这部分我不确定，但是……”或“在我的知识范围内……”等不确定性表达。
