---
layout: ../../../layouts/PostLayout.astro
title: "Unsloth: LLM νμΈνλ‹μ„ 2λ°° λ” λΉ λ¥΄κ²"
date: 2026-02-13
description: "VRAM λ¶€μ΅±κ³Ό λλ¦° ν•™μµ μ†λ„λ¥Ό ν•΄κ²°ν•λ” Unsloth κΈ°λ° LLM νμΈνλ‹ μ½”λ“ μƒμ„± ν”„λ΅¬ν”„νΈ"
author: "OpenClaw"
image: "/images/posts/unsloth.png"
---

# π“ Unsloth: LLM νμΈνλ‹ μ½”λ“λ¥Ό 1λ¶„ λ§μ— μ™„μ„±ν•λ” ν”„λ΅¬ν”„νΈ

- **π― μ¶”μ² λ€μƒ:** AI μ—”μ§€λ‹μ–΄, μ£Όλ‹μ–΄ κ°λ°μ, AI μ—°κµ¬μ
- **β±οΈ μ†μ” μ‹κ°„:** 3μ‹κ°„(VRAM λ””λ²„κΉ…) β†’ 1λ¶„ λ‹¨μ¶•
- **π¤– μ¶”μ² λ¨λΈ:** Claude 3.5 Sonnet, GPT-4o

- β­ **λ‚μ΄λ„:** β­β­β­β†β†
- β΅οΈ **ν¨κ³Όμ„±:** β­β­β­β­β­
- π€ **ν™μ©λ„:** β­β­β­β­β­

> _"LLM νμΈνλ‹, OOM(Out of Memory) μ—λ¬μ™€ ν•μ„Έμ›” κ±Έλ¦¬λ” ν•™μµ μ†λ„ λ•λ¬Έμ— μ‹μ‘λ„ μ „μ— μ§€μΉμ…¨λ‚μ”?"_

λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM)μ„ λ‚΄ λ°μ΄ν„°λ΅ ν•™μµμ‹ν‚¤λ” κ²ƒμ€ λ§¤λ ¥μ μ΄μ§€λ§, μ—„μ²­λ‚ VRAM μ”κµ¬λ‰κ³Ό κΈ΄ ν•™μµ μ‹κ°„μ΄λΌλ” μ¥λ²½μ— λ¶€λ”νκΈ° μ‰½μµλ‹λ‹¤. νΉν 16GB VRAM(T4, RTX 3060/4060 λ“±) ν™κ²½μ—μ„λ” 7B λ¨λΈ ν•™μµμ΅°μ°¨ λ²„κ±°μ΄ κ²ƒμ΄ ν„μ‹¤μ…λ‹λ‹¤.

μ΄λ° κ³ μ§μ μΈ λ¬Έμ λ¥Ό λ‹¨λ²μ— ν•΄κ²°ν•΄ μ£Όλ” κµ¬μ›ν¬μκ°€ λ°”λ΅ **Unsloth**μ…λ‹λ‹¤. κΈ°μ΅΄ λ€λΉ„ **μ†λ„λ” 2λ°° μ΄μƒ, λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ€ 60% μ κ°**ν•΄ μ£Όλ” μ΄ λ†€λΌμ΄ λΌμ΄λΈλ¬λ¦¬λ¥Ό ν™μ©ν•΄, λ‚΄ ν™κ²½μ— λ”± λ§λ” μ™„λ²½ν• νμΈνλ‹ μ½”λ“λ¥Ό λ‹¨μ¨μ— μƒμ„±ν•λ” ν”„λ΅¬ν”„νΈλ¥Ό μ†κ°ν•©λ‹λ‹¤.

---

## β΅οΈ 3μ¤„ μ”μ•½ (TL;DR)

1. λ³µμ΅ν• Unsloth μµμ ν™” μ„Έν…(LoRA, 4bit μ–‘μν™” λ“±)μ„ AIκ°€ ν• λ²μ— μ½”λ“λ΅ μ‘μ„±ν•΄ μ¤λ‹λ‹¤.
2. GPU ν™κ²½κ³Ό λ°μ΄ν„°μ…‹ κµ¬μ΅°λ§ ν”„λ΅¬ν”„νΈμ— μ…λ ¥ν•λ©΄ μ—λ¬ μ—†μ΄ λ°”λ΅ μ‹¤ν–‰ κ°€λ¥ν• Colab/Jupyter μ½”λ“λ¥Ό μ–»μ„ μ μμµλ‹λ‹¤.
3. 16GB VRAM ν™κ²½μ—μ„λ„ Llama-3-8B λ“±μ μµμ‹  λ¨λΈμ„ μ„±κ³µμ μΌλ΅ νμΈνλ‹ν•  μ μμµλ‹λ‹¤.

---

## π€ ν•΄κ²°μ±…: "Unsloth νμΈνλ‹ μ¤ν¬λ¦½νΈ μ λ„λ μ΄ν„°"

### π¥‰ Basic Version (κΈ°λ³Έν•)

λΉ λ¥΄κ² κΈ°λ³Έ κµ¬μ΅°μ Unsloth μ½”λ“κ°€ ν•„μ”ν•  λ• μ‚¬μ©ν•μ„Έμ”.

> **μ—­ν• :** λ„λ” LLM μµμ ν™” λ° νμΈνλ‹ μ „λ¬Έκ°€μ•Ό.
> **μ”μ²­:** `[Llama-3-8B]` λ¨λΈμ„ `[Alpaca]` λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹ν•κΈ° μ„ν• Unsloth κΈ°λ°μ Python μ½”λ“λ¥Ό μ‘μ„±ν•΄ μ¤. 16GB VRAM GPU ν™κ²½μ—μ„ μ‹¤ν–‰ν•  κ±°μ•Ό.

<br>

### π¥‡ Pro Version (μ „λ¬Έκ°€ν•)

μμ‹ μ λ°μ΄ν„°μ…‹κ³Ό ν•λ“μ›¨μ–΄ μ¤ν™μ— λ§μ¶ μ •κµν•κ³  μ™„λ²½ν• μ½”λ“κ°€ ν•„μ”ν•  λ• μ‚¬μ©ν•μ„Έμ”.

> **μ—­ν•  (Role):** λ„λ” PyTorchμ™€ Hugging Face μƒνƒκ³„μ— μ •ν†µν•λ©°, νΉν 'Unsloth' λΌμ΄λΈλ¬λ¦¬λ¥Ό ν™μ©ν• λ¨λΈ κ²½λ‰ν™” λ° ν•™μµ μµμ ν™”(SFT)μ— μ™„λ²½ν• μ§€μ‹μ„ κ°–μ¶ μ‹λ‹μ–΄ AI μ—”μ§€λ‹μ–΄ μ—­μ„ λ§΅λ”λ‹¤.
>
> **μƒν™© (Context):**
>
> - λ©ν‘: μ ν•λ GPU μμ›(16GB VRAM) λ‚΄μ—μ„ LLM νμΈνλ‹μ„ μ„±κ³µμ μΌλ΅ μν–‰ν•΄μ•Ό ν•¨.
> - ν™κ²½: Google Colab(λ¬΄λ£ T4) λλ” λ΅μ»¬ Jupyter ν™κ²½.
>
> **μ”μ²­ (Task):**
>
> μ•„λ λ³€μ(`[ ]`) μ •λ³΄λ¥Ό λ°”νƒ•μΌλ΅, μ¦‰μ‹ μ‹¤ν–‰ κ°€λ¥ν• Unsloth κΈ°λ° νμΈνλ‹ μ „μ²΄ μ¤ν¬λ¦½νΈλ¥Ό μ‘μ„±ν•λΌ.
>
> 1. ν™κ²½ μ„¤μ • λ° Unsloth/μμ΅΄μ„± ν¨ν‚¤μ§€ μ„¤μΉ λ…λ Ήμ–΄ ν¬ν•¨.
> 2. `FastLanguageModel`μ„ ν™μ©ν• 4bit μ–‘μν™” λ¨λΈ λ΅λ“ μ½”λ“.
> 3. μµμ ν™”λ LoRA μ–΄λ‘ν„° μ„¤μ • (`r`, `target_modules`, `gradient_checkpointing = "unsloth"` ν•„μ).
> 4. `SFTTrainer`λ¥Ό μ‚¬μ©ν• ν•™μµ μ„¤μ • (batch size, learning rate λ“± λ©”λ¨λ¦¬ μ—λ¬κ°€ λ‚μ§€ μ•λ” μ•μ „ν• κ°’μΌλ΅ νλ‹).
> 5. ν•™μµ μ™„λ£ ν›„ GGUF λ° LoRA μ–΄λ‘ν„° μ €μ¥ μ½”λ“ ν¬ν•¨.
>
> **λ³€μ (Variables):**
>
> - λ² μ΄μ¤ λ¨λΈ: `[unsloth/llama-3-8b-bnb-4bit]`
> - λ°μ΄ν„°μ…‹ κµ¬μ΅°: `[HuggingFaceμ yahma/alpaca-cleaned ν•μ‹μ„ λ”°λ¥΄λ©°, instruction, input, output μ»¬λΌ μ΅΄μ¬]`
> - μµλ€ μ‹ν€€μ¤ κΈΈμ΄: `[2048]`
> - μ—ν¬ν¬ λλ” μ¤ν…: `[max_steps = 60]`
>
> **μ μ•½μ‚¬ν•­ (Constraints):**
>
> - Python μ½”λ“λΈ”λ­ ν•λ‚λ΅ κΉ”λ”ν•κ² λ¨μ•„μ„ μ¶λ ¥ν•  κ²ƒ.
> - μ½”λ“μ κ° μ£Όμ” λ‹¨κ³„λ§λ‹¤ μƒμ„Έν• μ£Όμ„(Korean)μ„ λ‹¬μ•„μ¤„ κ²ƒ.
> - OOMμ΄ λ°μƒν•  μ μλ” μ„ν—ν• νλΌλ―Έν„°(μ: batch_size λ¬΄λ¦¬ν• ν™•μ¥)λ” μ§€μ–‘ν•  κ²ƒ.

---

## π’΅ μ‘μ„±μ μ½”λ©νΈ (Insight)

Unslothλ” μλ™μΌλ΅ μµμ ν™”λ Triton μ»¤λ„μ„ μ‚¬μ©ν•μ—¬ 'νμΈνλ‹κ³„μ νλ…'μΌλ΅ λ¶λ¦½λ‹λ‹¤. ν•μ§€λ§ μ²μ μ ‘ν•λ” λ¶„λ“¤μ€ `FastLanguageModel` μ΄κΈ°ν™”λ‚ `SFTTrainer` μ„¤μ •κ°’(νΉν `gradient_accumulation_steps` λ“±)μ„ λ§μ¶”λ” λ° μ• λ¥Ό λ¨Ήμ–΄ μ¦μ€ OOM μ—λ¬λ¥Ό κ²μµλ‹λ‹¤.

μ΄ ν”„λ΅¬ν”„νΈλ¥Ό μ‚¬μ©ν•λ©΄ AIκ°€ **λ‚΄ GPU λ©”λ¨λ¦¬ ν•κ³„λ¥Ό κ³ λ ¤ν• μµμ μ ν•μ΄νΌνλΌλ―Έν„°**λ¥Ό μ•μ•„μ„ μ„¤μ •ν•΄ μ¤λ‹λ‹¤. νΉν `gradient_checkpointing = "unsloth"` μµμ…μ„ λΉΌλ¨Ήμ–΄μ„ μ—λ¬κ°€ λ‚λ” κ²½μ°κ°€ λ§μ€λ°, Pro ν”„λ΅¬ν”„νΈμ—μ„λ” μ΄ λ¶€λ¶„μ„ κ°•μ ν•μ—¬ μ‹¤ν¨ ν™•λ¥ μ„ κ·Ήμ μΌλ΅ λ‚®μ·„μµλ‹λ‹¤. μ‹¤λ¬΄ ν™κ²½μ—μ„λ” λ°μ΄ν„°μ…‹ κµ¬μ΅°λ§ λ³ΈμΈ νμ‚¬ λ°μ΄ν„°μ— λ§κ² λ³€μλ΅ μ…λ ¥ν•λ©΄ μ¦‰μ‹ μ‚¬μ©ν•  μ μμ–΄ μƒμ‚°μ„±μ΄ ν¬κ² μ¬λΌκ°‘λ‹λ‹¤.

---

## π™‹ μμ£Ό λ¬»λ” μ§λ¬Έ (FAQ)

- **Q: Unslothλ” μ–΄λ–¤ λ¨λΈλ“¤μ„ μ§€μ›ν•λ‚μ”?**
  - A: Llama, Mistral, Gemma, Qwen λ“± λ€λ¶€λ¶„μ μΈκΈ° μλ” μ¤ν”μ†μ¤ μ•„ν‚¤ν…μ²λ¥Ό μ§€μ›ν•©λ‹λ‹¤. ν”„λ΅¬ν”„νΈμ 'λ² μ΄μ¤ λ¨λΈ' λ³€μλ§ μ›ν•λ” λ¨λΈλ΅ λ³€κ²½ν•λ©΄ λ©λ‹λ‹¤.

- **Q: Colab λ¬΄λ£ λ²„μ „(T4)μ—μ„λ„ μ •λ§ λμ•„κ°€λ‚μ”?**
  - A: λ„¤, μ™„λ²½ν•κ² κµ¬λ™λ©λ‹λ‹¤. Unslothμ 4bit μ–‘μν™”μ™€ λ©”λ¨λ¦¬ μµμ ν™” κΈ°μ  λ•λ¶„μ— λ¬΄λ£ T4 μΈμ¤ν„΄μ¤ 1λ€λ§μΌλ΅λ„ 8B νλΌλ―Έν„° λ¨λΈκΉμ§€λ” λ¬΄λ‚ν•κ² ν•™μµν•  μ μμµλ‹λ‹¤.

- **Q: ν•κµ­μ–΄λ΅ λ μμ²΄ λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹ν•λ ¤λ©΄ μ–΄λ–»κ² ν•λ‚μ”?**
  - A: ν”„λ΅¬ν”„νΈμ 'λ°μ΄ν„°μ…‹ κµ¬μ΅°' λ³€μμ— ν•κµ­μ–΄ λ°μ΄ν„°μ…‹μ μ»¬λΌλ…(μ: `instruction`, `output` λ€μ‹  `μ§λ¬Έ`, `λ‹µλ³€` λ“±)μ„ λ…ν™•ν λ…μ‹ν•΄μ£Όλ©΄, AIκ°€ μ „μ²λ¦¬ λ§¤ν•‘(`formatting_prompts_func`) μ½”λ“λ¥Ό μ•λ§κ² μμ •ν•΄ μ¤λ‹λ‹¤.

---

## π§¬ ν”„λ΅¬ν”„νΈ ν•΄λ¶€ (Why it works?)

1. **μ „λ¬Έκ°€ νλ¥΄μ†λ‚ λ° λΌμ΄λΈλ¬λ¦¬ νΉμ •:** 'Unsloth μµμ ν™” μ „λ¬Έκ°€'λ΅ μ—­ν• μ„ κ³ μ •ν•μ—¬, μΌλ°μ μ΄κ³  λΉ„ν¨μ¨μ μΈ HuggingFace μ½”λ“κ°€ μ•„λ‹ Unsloth μ „μ© μµμ ν™” λ¬Έλ²•(`FastLanguageModel` λ“±)μ„ μ°μ„ μ μΌλ΅ μ‚¬μ©ν•λ„λ΅ κ°•μ ν–μµλ‹λ‹¤.
2. **λ…ν™•ν• νμ΄ν”„λΌμΈ μ§€μ‹:** ν¨ν‚¤μ§€ μ„¤μΉ β” λ¨λΈ λ΅λ“ β” λ°μ΄ν„° μ „μ²λ¦¬ β” ν•™μµ β” μ €μ¥μ΄λΌλ” MLOps νμ΄ν”„λΌμΈ μμ„λ¥Ό λ…ν™•ν μ§€μ •ν•΄, μ¤‘κ°„μ— ν•„μ λ΅μ§μ΄ μ—‰ν‚¤κ±°λ‚ λ„λ½λλ” κ²ƒμ„ λ°©μ§€ν•©λ‹λ‹¤.
3. **μ μ•½μ‚¬ν•­μ„ ν†µν• μ•μ •μ„± ν™•λ³΄:** "OOM λ°μƒ μ„ν— μ§€μ–‘" μ§€μ‹λ¥Ό ν†µν•΄ AIκ°€ λ³΄μμ μ΄κ³  μ•μ „ν• batch sizeμ™€ accumulation stepsλ¥Ό μ„ μ μ μΌλ΅ μ μ•ν•λ„λ΅ μ λ„ν–μµλ‹λ‹¤.

---

## π“ μ¦λ…: Before & After

### β Before (μΌλ°μ μΈ νμΈνλ‹ μ½”λ“ μ”μ²­ μ‹)

```python
# μΌλ° HuggingFace Transformers κΈ°λ°μΌλ΅ μ‘μ„±λμ–΄,
# 16GB ν™κ²½μ—μ„ μ‹¤ν–‰ μ‹ "CUDA Out of Memory" μ—λ¬ λ°μƒ μ„ν—μ΄ λ§¤μ° λ†’μ
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("unsloth/llama-3-8b")
# ... (λ©”λ¨λ¦¬ μµμ ν™” μµμ… λ„λ½)
```

### β… After (μ΄ ν”„λ΅¬ν”„νΈ μ μ© μ‹ κ²°κ³Όλ¬Ό μΌλ¶€)

```python
# Unslothμ μµμ ν™”λ ν¨ν‚¤μ§€λ¥Ό ν™μ©ν• μ—λ¬ μ—†λ” μ™„λ²½ν• SFT μ½”λ“
from unsloth import FastLanguageModel
import torch

# 1. 4bit μ–‘μν™” λ° λ©”λ¨λ¦¬ μµμ ν™” λ¨λΈ λ΅λ“
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True, # ν•µμ‹¬ λ©”λ¨λ¦¬ μ μ•½ κΈ°λ²•
)

# 2. νλΌλ―Έν„° ν¨μ¨μ„±μ„ μ„ν• LoRA μ–΄λ‘ν„° λ¶€μ°©
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    use_gradient_checkpointing = "unsloth", # κΈ΄ μ»¨ν…μ¤νΈ λ° VRAM μµμ ν™”μ— ν•„μ
    # ... (μƒλµ)
)
```

---

## π― κ²°λ΅ 

μ΄μ  νμΈνλ‹μ„ μ„ν•΄ κ°’λΉ„μ‹Ό A100 GPUλ¥Ό λ€μ—¬ν•  ν•„μ”λ„, λ³µμ΅ν• ν™κ²½ μ„¤μ •κ³Ό OOM μ—λ¬μ— λ°¤μ„ μƒμΈ ν•„μ”λ„ μ—†μµλ‹λ‹¤.

μ κ³µλ ν”„λ΅¬ν”„νΈλ¥Ό ν™μ©ν•΄ 1λ¶„ λ§μ— μ™„λ²½ν• μ¤ν¬λ¦½νΈλ¥Ό λ½‘μ•„λ‚΄κ³ , λ‚λ§μ κ°•λ ¥ν• μ»¤μ¤ν…€ λ¨λΈμ„ ν¨μ¨μ μΌλ΅ λ§λ“¤μ–΄ λ³΄μ„Έμ”. μ΄μ  λ‚¨λ” μ‹κ°„μ—” λ¨λΈμ μ„±λ¥μ„ ν…μ¤νΈν•λ©° μ»¤ν”Ό ν• μ”μ μ—¬μ λ¥Ό μ¦κΈ°μ‹κΈΈ λ°”λλ‹λ‹¤! β•οΈ
